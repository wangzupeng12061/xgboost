# 因子数据缓存功能说明

## 概述

为了支持直接加载已训练模型进行回测，系统现在会自动保存计算好的因子数据到缓存目录。

## 功能说明

### 1. 自动保存因子数据

在运行 `main.py` 完整流程时，系统会在**标签构建完成后**自动保存因子数据：

```python
# 位置：main.py, 标签构建步骤之后
cache_dir = './cache'
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

# 保存带时间戳的版本
cache_path = f"{cache_dir}/factors_{timestamp}.parquet"
labeled_data.to_parquet(cache_path, index=False)

# 保存最新版本（覆盖）
latest_cache_path = f"{cache_dir}/factors_latest.parquet"
labeled_data.to_parquet(latest_cache_path, index=False)
```

### 2. 保存的文件

**位置**: `./cache/`

**文件格式**:
- `factors_YYYYMMDD_HHMMSS.parquet` - 带时间戳的因子数据
- `factors_latest.parquet` - 最新的因子数据（方便快速加载）

**文件内容**:
- 所有计算好的因子列
- 价格数据（open, high, low, close等）
- 标签（label）
- 前向收益率（forward_return）
- 股票代码和日期

### 3. 使用缓存进行回测

使用 `run_backtest_with_model.py` 脚本时，会自动加载缓存的因子数据：

```bash
# 使用最新保存的模型和因子数据
python run_backtest_with_model.py

# 使用指定的模型文件
python run_backtest_with_model.py results/models/model_20251104_214332.pkl
```

**加载逻辑**:
1. 优先加载 `factors_latest.parquet`
2. 如果不存在，查找最新的带时间戳的文件
3. 如果都没有，提示用户先运行完整流程

## 使用流程

### 完整流程（首次使用）

```bash
# 1. 运行完整流程（会自动保存因子数据和模型）
python main.py

# 输出:
# ✓ 因子数据已保存: ./cache/factors_20251104_223000.parquet
# ✓ 最新因子数据: ./cache/factors_latest.parquet
# ✓ 模型已保存至: results/models/model_20251104_223000.pkl
```

### 快速回测（使用已保存数据）

```bash
# 2. 使用已保存的模型和因子数据进行回测
python run_backtest_with_model.py

# 输出:
# 自动选择最新模型: model_20251104_223000.pkl
# 从缓存加载: factors_latest.parquet
# ✓ 因子数据加载完成: 150000 条记录
```

## 优点

### 1. **节省时间**
- 跳过数据加载（1-2分钟）
- 跳过因子计算（2-5分钟）
- 跳过因子处理和筛选（1-2分钟）
- **总计可节省5-10分钟**

### 2. **保持一致性**
- 模型训练和回测使用完全相同的因子数据
- 避免因子计算差异导致的不一致
- 确保结果可复现

### 3. **便于对比**
- 可以使用相同的因子数据测试不同的模型
- 可以使用相同的模型测试不同时期的数据

### 4. **支持离线使用**
- 因子数据缓存后，回测不需要网络连接
- 不需要重新调用API获取数据

## 文件大小

- Parquet格式使用列式存储和压缩
- 典型大小：
  - 1000只股票 × 5年 × 20个因子 ≈ 50-100 MB
  - 100只股票 × 2年 × 20个因子 ≈ 5-10 MB

## 缓存管理

### 查看缓存

```bash
ls -lh cache/
```

### 清理旧缓存

```bash
# 只保留最新的
rm cache/factors_2025*.parquet

# 保留 factors_latest.parquet
```

### 手动加载缓存

```python
import pandas as pd

# 加载最新缓存
factor_data = pd.read_parquet('./cache/factors_latest.parquet')

# 查看数据信息
print(f"样本数: {len(factor_data)}")
print(f"列名: {factor_data.columns.tolist()}")
print(f"日期范围: {factor_data['date'].min()} 至 {factor_data['date'].max()}")
```

## 注意事项

### 1. 数据时效性

- 缓存的因子数据基于特定时间点的原始数据
- 如果市场数据更新，需要重新运行 `main.py` 生成新的缓存

### 2. 配置变化

以下配置变化需要重新生成因子数据：
- 因子计算方法修改
- 因子筛选条件修改
- 标签构建参数修改
- 数据时间范围修改

### 3. 磁盘空间

- 每次运行会生成新的带时间戳的文件
- 定期清理旧文件以节省空间
- `factors_latest.parquet` 始终保持最新

## 故障排查

### 问题1: 未找到因子数据缓存

**错误信息**:
```
ERROR - 未找到因子数据缓存！
INFO - 请先运行 main.py 生成因子数据
```

**解决方法**:
```bash
# 运行完整流程生成缓存
python main.py
```

### 问题2: 缓存数据损坏

**错误信息**:
```
ParquetInvalidOrCorruptedError
```

**解决方法**:
```bash
# 删除损坏的缓存
rm cache/factors_latest.parquet

# 重新生成
python main.py
```

### 问题3: 特征不匹配

**错误信息**:
```
WARNING - 缺失特征: {'factor_x', 'factor_y'}
```

**原因**: 模型训练时使用的因子与缓存数据中的因子不一致

**解决方法**:
1. 使用匹配的模型和因子数据
2. 或者重新训练模型使用当前的因子数据

## 最佳实践

### 1. 版本管理

```bash
# 为重要的模型和因子数据创建版本备份
cp cache/factors_latest.parquet cache/factors_v1.0_stable.parquet
cp results/models/model_20251104_223000.pkl results/models/model_v1.0_stable.pkl
```

### 2. 数据验证

运行回测前验证数据：
```python
import pandas as pd

factor_data = pd.read_parquet('./cache/factors_latest.parquet')

# 检查数据完整性
assert 'label' in factor_data.columns, "缺少标签列"
assert factor_data['date'].min() <= pd.to_datetime('2020-01-01'), "数据起始日期异常"
assert not factor_data.isnull().any().any(), "存在缺失值"

print("✓ 数据验证通过")
```

### 3. 文档记录

记录每次生成的因子数据：
```bash
# 创建README
cat > cache/README.txt << EOF
因子数据缓存目录
生成时间: 2025-11-04 22:30:00
数据范围: 2020-01-01 至 2025-11-04
股票数量: 1000
因子数量: 21
样本总数: 150000
EOF
```

## 总结

因子数据缓存功能大大提高了回测效率，使得可以快速测试不同的模型参数或回测策略，而无需重复进行耗时的数据处理和因子计算步骤。

**关键要点**:
- ✅ 自动保存：运行 `main.py` 自动保存因子数据
- ✅ 快速加载：`run_backtest_with_model.py` 自动加载缓存
- ✅ 保持同步：确保模型和因子数据匹配
- ✅ 定期清理：避免占用过多磁盘空间

---
更新日期: 2025-11-04
